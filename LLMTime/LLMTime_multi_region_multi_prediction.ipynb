{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import ollama"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###Helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters\n",
    "PREDICTION_MODE = \"end_volume\"\n",
    "N_PREDICTED_VALUES = 20 \n",
    "TEMPERATURE = 0.8\n",
    "TOP_P = 0.9\n",
    "OUTPUT_FOLDER = \"predictions\"\n",
    "\n",
    "# Create output folder if it doesn't exist\n",
    "if not os.path.exists(OUTPUT_FOLDER):\n",
    "    os.makedirs(OUTPUT_FOLDER)\n",
    "\n",
    "# Normalization function\n",
    "def normalize_data(data):\n",
    "    return (data - np.min(data)) / (np.max(data) - np.min(data))\n",
    "\n",
    "# Tokenization function\n",
    "def tokenize_data(data):\n",
    "    # Replace NaN values with 0\n",
    "    data = np.nan_to_num(data, nan=0.0)\n",
    "    # Tokenize by scaling to 0-99 range and converting to formatted strings\n",
    "    return \", \".join(f\"{int(value * 100):02d}\" for value in data)\n",
    "\n",
    "# Parse predicted values\n",
    "def parse_predicted_values(predicted_values):\n",
    "    return [int(value) / 100 for value in predicted_values.split(\", \") if value]\n",
    "\n",
    "# Save predictions to CSV\n",
    "def save_predictions(region_id, predictions, ground_truth, mode):\n",
    "    file_name = f\"{OUTPUT_FOLDER}/predictions_{mode}_region_{region_id}.csv\"\n",
    "    df = pd.DataFrame({\n",
    "        'Prediction': predictions,\n",
    "        'Ground_Truth': ground_truth\n",
    "    })\n",
    "    df.to_csv(file_name, index=False)\n",
    "    # print(f\"Saved predictions for region {region_id} to {file_name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###Ollama prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_predictions(input_text):\n",
    "    # Prompt for prediction\n",
    "    # print(\"Tokenized data:\", input_text[:50])\n",
    "    max_chars_output = N_PREDICTED_VALUES * 4\n",
    "\n",
    "    # LLaMA model\n",
    "    response = ollama.chat(model='llama2:13b-text', messages=[\n",
    "        {\n",
    "            'role': 'user',\n",
    "            'content': input_text,\n",
    "        },\n",
    "    ], options={\n",
    "        \"temperature\": TEMPERATURE,\n",
    "        \"top_p\": TOP_P,\n",
    "        \"num_predict\": max_chars_output\n",
    "    })\n",
    "\n",
    "    # Extract the predicted values\n",
    "    predicted_values = response['message']['content']\n",
    "    # print(\"Predicted values:\", predicted_values)\n",
    "\n",
    "    # Parse the predictions into a list\n",
    "    return parse_predicted_values(predicted_values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###Main Processing Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_regions(data, prediction_mode, test_region=None, regions=None):\n",
    "    if test_region is not None:\n",
    "        regions = [test_region]\n",
    "    elif regions is None:\n",
    "        regions = data['region'].unique()\n",
    "    \n",
    "    regions = [region for region in regions if region >= 141]\n",
    "    print(f\"Processing regions: {regions}\")\n",
    "\n",
    "    for region_id in regions:\n",
    "        print(f\"Processing region {region_id}...\")\n",
    "        # Extract data for the region\n",
    "        # region_data = data[data['region'] == region_id][prediction_mode]\n",
    "        region_data = data[data['region'] == region_id][prediction_mode].values \n",
    "\n",
    "        # print(\"--- Region data ---\")\n",
    "        # print(region_data)\n",
    "        \n",
    "        # Ensure sufficient data\n",
    "        if len(region_data) < 483 + N_PREDICTED_VALUES:\n",
    "            print(f\"Region {region_id} does not have enough data. Skipping.\")\n",
    "            continue\n",
    "\n",
    "        # Normalize data\n",
    "        normalized_data = normalize_data(region_data)\n",
    "\n",
    "        # print(\"--- Normalized data ---\")\n",
    "        # print(normalized_data)\n",
    "\n",
    "        # Select first 483 timeslots to use as input for the model\n",
    "        input_data = normalized_data[:483]\n",
    "        tokenized_input = tokenize_data(input_data)\n",
    "\n",
    "        # print(\"--- Tokenized input ---\")\n",
    "        # print(tokenized_input)\n",
    "\n",
    "        # Save ground truth (484th value)\n",
    "        ground_truth = normalized_data[483:483 + N_PREDICTED_VALUES].tolist()\n",
    "\n",
    "        # Note: the bike dataset had many regions with 0.0 values, thus I added this check \n",
    "        # the normalization function will return nan for all 0.0 values (due to division by 0)\n",
    "        # in order to save computation (and we really needed that!), if the ground truth are all 0.0 or nan\n",
    "        # we can skip the region since the LLaMa predictions will be 0.0 (we will filter out the values below 10 in our eval anyway!)\n",
    "        # all the taxi runs and bike 1/2/3 did not have this check, so based on the empirical results, the predictions were not useful\n",
    "        # i.e. LLaMa would never predict \"87\" if the ground truth was 0.0!\n",
    "        if all(value == 0.0 or np.isnan(value) for value in ground_truth):\n",
    "            print(f\"Ground truth NaN. Skipping region {region_id}.\")\n",
    "            predictions = [0.0] * N_PREDICTED_VALUES\n",
    "            ground_truth = [0.0] * N_PREDICTED_VALUES\n",
    "            save_predictions(region_id, predictions, ground_truth, prediction_mode)\n",
    "            continue\n",
    "\n",
    "        # Make predictions (20 predictions for the 484th value)\n",
    "        predictions = make_predictions(tokenized_input)\n",
    "\n",
    "        # Save predictions\n",
    "        save_predictions(region_id, predictions, ground_truth, prediction_mode)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###Load and Execute"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing regions: [np.int64(141), np.int64(142), np.int64(143), np.int64(144), np.int64(145), np.int64(146), np.int64(147), np.int64(148), np.int64(149), np.int64(150), np.int64(151), np.int64(152), np.int64(153), np.int64(154), np.int64(155), np.int64(156), np.int64(157), np.int64(158), np.int64(159), np.int64(160), np.int64(161), np.int64(162), np.int64(163), np.int64(164), np.int64(165), np.int64(166), np.int64(167), np.int64(168), np.int64(169), np.int64(170), np.int64(171), np.int64(172), np.int64(173), np.int64(174), np.int64(175), np.int64(176), np.int64(177), np.int64(178), np.int64(179), np.int64(180), np.int64(181), np.int64(182), np.int64(183), np.int64(184), np.int64(185), np.int64(186), np.int64(187), np.int64(188), np.int64(189), np.int64(190), np.int64(191), np.int64(192), np.int64(193), np.int64(194), np.int64(195), np.int64(196), np.int64(197), np.int64(198), np.int64(199)]\n",
      "Processing region 141...\n",
      "Processing region 142...\n",
      "Processing region 143...\n",
      "Processing region 144...\n",
      "Processing region 145...\n",
      "Processing region 146...\n",
      "Processing region 147...\n",
      "Processing region 148...\n",
      "Processing region 149...\n",
      "Processing region 150...\n",
      "Processing region 151...\n",
      "Processing region 152...\n",
      "Processing region 153...\n",
      "Processing region 154...\n",
      "Processing region 155...\n",
      "Processing region 156...\n",
      "Processing region 157...\n",
      "Processing region 158...\n",
      "Processing region 159...\n",
      "Ground truth NaN. Skipping region 159.\n",
      "Processing region 160...\n",
      "Processing region 161...\n",
      "Processing region 162...\n",
      "Ground truth NaN. Skipping region 162.\n",
      "Processing region 163...\n",
      "Processing region 164...\n",
      "Processing region 165...\n",
      "Processing region 166...\n",
      "Processing region 167...\n",
      "Processing region 168...\n",
      "Processing region 169...\n",
      "Processing region 170...\n",
      "Processing region 171...\n",
      "Processing region 172...\n",
      "Processing region 173...\n",
      "Processing region 174...\n",
      "Processing region 175...\n",
      "Processing region 176...\n",
      "Processing region 177...\n",
      "Processing region 178...\n",
      "Processing region 179...\n",
      "Processing region 180...\n",
      "Processing region 181...\n",
      "Processing region 182...\n",
      "Processing region 183...\n",
      "Processing region 184...\n",
      "Processing region 185...\n",
      "Processing region 186...\n",
      "Processing region 187...\n",
      "Processing region 188...\n",
      "Processing region 189...\n",
      "Processing region 190...\n",
      "Processing region 191...\n",
      "Processing region 192...\n",
      "Processing region 193...\n",
      "Processing region 194...\n",
      "Processing region 195...\n",
      "Processing region 196...\n",
      "Processing region 197...\n",
      "Processing region 198...\n",
      "Processing region 199...\n"
     ]
    }
   ],
   "source": [
    "# Load the traffic volume dataset\n",
    "file_path = 'llmtime_data/taxi_volume_test.csv'\n",
    "data = pd.read_csv(file_path)\n",
    "\n",
    "# Note: to process a single region, set test_region_id  otherwise set it to None\n",
    "test_region_id = None\n",
    "\n",
    "# print(data.head())\n",
    "\n",
    "process_regions(data, PREDICTION_MODE, test_region=test_region_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: computation time on Mac M2 Pro was 67min on average for each run"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
